{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOGLE COLLAB\n",
    "_='''\n",
    "! rm -rf __MACOSX/ landscapes data\n",
    "! mkdir data\n",
    "! curl https://s3.amazonaws.com/artemis.ai/landscapes.zip --output images.zip\n",
    "! unzip images.zip\n",
    "! rm -rf __MACOSX/\n",
    "! mv landscapes data/landscapes\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from time import time\n",
    "from skimage import io, color\n",
    "from torchvision import datasets\n",
    "from skimage.color import lab2rgb, rgb2lab, rgb2gray\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install split-folders\n",
    "import splitfolders\n",
    "# Split with a ratio.\n",
    "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "splitfolders.ratio(\"data/landscapes\", output=\"output\", seed=1337, ratio=(.8, .2), group_prefix=None) # default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GrayscaleImageFolder(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        # Get rgb / lab\n",
    "        rgb = self.loader(path)\n",
    "        rgb = self.transform(rgb)\n",
    "        rgb = np.asarray(rgb)\n",
    "        lab = color.rgb2lab(rgb)\n",
    "        #lab = (lab + 128) / 255\n",
    "        \n",
    "        gray, lab = lab[:,:,0], lab[:, :, 1:3]\n",
    "        \n",
    "        # Transpose from WxHxC to CxWxH\n",
    "        rgb = np.transpose(rgb, (2, 0, 1))\n",
    "        lab = np.transpose(lab, (2, 0, 1))\n",
    "        \n",
    "        rgb = torch.from_numpy(rgb).float() / 255\n",
    "        gray = torch.from_numpy(gray).unsqueeze(0).float()\n",
    "        lab = torch.from_numpy(lab).float()\n",
    "        \n",
    "        return rgb, lab, gray\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Grayscale(),\n",
    "    transforms.Resize((256,256)),\n",
    "    #transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = GrayscaleImageFolder(\"./output/train\", transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=0)\n",
    "\n",
    "dataiter = iter(data_loader)\n",
    "rgb, lab, gray = dataiter.next()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True, figsize=(10,4))\n",
    "\n",
    "axes[0].imshow(np.transpose(rgb[0], (1, 2, 0)))\n",
    "axes[1].imshow(gray[0][0], cmap='gray')\n",
    "axes[2].imshow(lab[0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Grayscale(),\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_gray = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# load the training and test datasets\n",
    "dataset = datasets.ImageFolder(\"./output/train\", transform=transform)\n",
    "dataset_gray = datasets.ImageFolder(\"./output/train\", transform=transform_gray)\n",
    "\n",
    "dataset_test = datasets.ImageFolder(\"./output/val\", transform=transform)\n",
    "dataset_gray_test = datasets.ImageFolder(\"./output/val\", transform=transform_gray)\n",
    "#train_data = datasets.CelebA(root='data', download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test dataloaders\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 64\n",
    "\n",
    "# prepare data loaders\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "data_gray_loader = torch.utils.data.DataLoader(dataset_gray, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, num_workers=num_workers)\n",
    "data_gray_loader_test = torch.utils.data.DataLoader(dataset_gray_test, batch_size=batch_size, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first image\n",
    "dataiter = iter(data_loader)\n",
    "images, labels = dataiter.next()\n",
    "plt.imshow(np.transpose(dataset[0][0].numpy(), (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(data_gray_loader)\n",
    "images, labels = dataiter.next()\n",
    "plt.imshow(dataset_gray[0][0][0].numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the NN architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        # Batch norm\n",
    "        self.conv2_bn = nn.BatchNorm2d(24)\n",
    "        \n",
    "        # First conv layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, 7, padding=3)\n",
    "        #self.conv2 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        #self.conv3 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        # First inception block\n",
    "        self.conv1_1 = nn.Conv2d(32, 8, 5, padding=2)\n",
    "        self.conv1_2 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "        self.conv1_3 = nn.Conv2d(8, 8, 1, padding=0)\n",
    "        # Second inception block\n",
    "        self.conv2_1 = nn.Conv2d(24, 8, 5, padding=2)\n",
    "        self.conv2_2 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "        self.conv2_3 = nn.Conv2d(8, 8, 1, padding=0)\n",
    "        \n",
    "        # Final conv layer\n",
    "        self.conv4 = nn.Conv2d(24,3, 1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #x = F.relu(self.conv2(x))\n",
    "        #x = F.relu(self.conv3(x))\n",
    "        # First inception block\n",
    "        x1_1 = F.relu(self.conv1_1(x))\n",
    "        x1_2 = F.relu(self.conv1_2(x1_1))\n",
    "        x1_3 = F.relu(self.conv1_3(x1_2))\n",
    "        block1 = [x1_1, x1_2, x1_3]\n",
    "        x = torch.cat(block1, 1)\n",
    "        x = self.conv2_bn(x)\n",
    "        # Second inception block\n",
    "        x2_1 = F.relu(self.conv2_1(x))\n",
    "        x2_2 = F.relu(self.conv2_2(x2_1))\n",
    "        x2_3 = F.relu(self.conv2_3(x2_2))\n",
    "        x = torch.cat((x2_1, x2_2, x2_3), 1)\n",
    "        \n",
    "        x = F.sigmoid(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = ConvAutoencoder()\n",
    "\n",
    "# Testing purposes \n",
    "X = torch.rand((1,1,64,64))\n",
    "Y = model(X)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Using {[\"CPU\", \"CUDA\"][torch.cuda.is_available()]}')\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if cuda_available:\n",
    "    model = model.cuda()\n",
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 1\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    \n",
    "    for data, data_gray in zip(data_loader, data_gray_loader):\n",
    "        #start = time()\n",
    "        # Get both images, images_gray is the input, images is the expected output\n",
    "        images_gray, _ = data_gray\n",
    "        images, _ = data\n",
    "\n",
    "        if cuda_available:\n",
    "            images_gray = images_gray.cuda()\n",
    "            images = images.cuda()\n",
    "\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images_gray)\n",
    "       \n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, images)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "        \n",
    "        #print(time() - start)\n",
    "        #break\n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(data_loader)\n",
    "    print(\n",
    "        'Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 5\n",
    "model.cpu()\n",
    "dataiter = iter(data_gray_loader_test)\n",
    "X, _ = dataiter.next()[:5]\n",
    "Y = model(X).cpu().detach().numpy()\n",
    "\n",
    "dataiter = iter(data_loader_test)\n",
    "X, _ = dataiter.next()\n",
    "fig, axes = plt.subplots(nrows=2, ncols=n_inputs, sharex=True, sharey=True, figsize=(10,4))\n",
    "for i in range(n_inputs):\n",
    "    x = np.transpose(X[i+26], (1, 2, 0))\n",
    "    y = np.transpose(Y[i+26], (1, 2, 0))\n",
    "    axes[0][i].imshow(x)\n",
    "    axes[1][i].imshow(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
